
%% bare_conf.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[conference]{IEEEtran}
% Add the compsoc option for Computer Society conferences.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{algorithm,algorithmic}




% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.





% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )




% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}



\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{PESC: A Parallel System for Clustering ECG Streams Based on MapReduce}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Lin Yang, Jin Zhang, Qian Zhang}
\IEEEauthorblockA{Department of Computer Science and Engineering\\
Hong Kong University of Science and Technology, Hong Kong\\
Email: \{lyangab, jinzh, qianzh\}@ust.hk}
}

% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
%\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
%Homer Simpson\IEEEauthorrefmark{2},
%James Kirk\IEEEauthorrefmark{3}, 
%Montgomery Scott\IEEEauthorrefmark{3} and
%Eldon Tyrell\IEEEauthorrefmark{4}}
%\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
%Georgia Institute of Technology,
%Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
%\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
%Email: homer@thesimpsons.com}
%\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
%Telephone: (800) 555--1212, Fax: (888) 555--1212}
%\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}




% make the title area
\maketitle


\begin{abstract}
\boldmath
Nowadays, cardiovascular disease (CVD) has become a disease of the majority. As an important instrument for diagnosing CVD, electrocardiography (ECG) is used to extract useful information about the functioning status of the heart. In the domain of ECG analysis, cluster analysis is a commonly applied approach to gain an overview of the data, detect outliers or pre-process before further analysis. In recent years, to provide better medical care for CVD patients, the cardiac telehealth system has been widely used. However, the extremely large volume and high update rate of data in the telehealth system has made cluster analysis challenging work. In this paper, we design and implement a novel parallel system for clustering massive ECG stream data based on the MapReduce framework. In our approach, a global optimum of clustering is achieved by merging and splitting clusters dynamically. Meanwhile, a good performance is gained by distributing computation over multiple computing nodes. According to the 
evaluation, our system not only provides good clustering results but also has an excellent performance on multiple computing nodes.
\end{abstract}
% IEEEtran.cls defaults to using nonbold math in the Abstract.
% This preserves the distinction between vectors and scalars. However,
% if the conference you are submitting to favors bold math in the abstract,
% then you can use LaTeX's standard command \boldmath at the very start
% of the abstract to achieve this. Many IEEE journals/conferences frown on
% math in the abstract anyway.

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% no \IEEEPARstart
% You must have at least 2 lines in the paragraph with the drop letter
% (should never be an issue)

% CVD is prevalent->ECG could help->many system have been proposed->the rise of cardiac telehealth system->the basic idea of cardiac telehealth system
According to the latest research from the medical society, cardiovascular disease (CVD) has become a disease of the majority. Nowadays, more than one in three people are suffering this kind of disease in the United States \cite{Roger15122011}. As an important instrument for diagnosing CVD, the electrocardiography (ECG), a transthoracic interpretation of the electrical activity of the heart over a period of time, can be used to extract useful information about the functional status of the heart \cite{springerlink:10.1007/978-3-642-15020-3_21}. To help clinicians better utilize the ECG data, a variety of systems have been proposed, especially in recent years, as healthcare costs continue to increase, resources within the health sector are being redirected from hospital based care to home based care, which drives the rise of cardiac telehealth system \cite{demiris2010role}. The cardiac telehealth systems are able to provide a high-quality, personalized, real-time and long-term ambulatory monitoring service for chronic CVD patients, while overcome barriers of time, cost, and distance. The cardiac telehealth system often consists in outfitting patients with portable, miniaturized and wireless sensors and devices that are capable to measure and report cardiac signals to telehealth providers. Owing to the fact that some chronic CVDs have low prevalence but high risk, the new-generation cardiac telehealth system prefers to transmit the ECG data to a remote server via wireless networks to provide high-quality interaction and real-time intervention for adverse cardiac events. 


%Traditional ECG clustering algorithms and their drawbacks
After the ECG data is collected, complex analysis will be performed to extract useful information. In this domain, clustering is a commonly applied approach to gain an overview of data, detect outliers or pre-process before further analysis, \textit{i.e.,} clustering could help clinicians to gain an overview of the ECG data, and locate abnormal cardiac events quickly without reviewing all the data manually. As an important research topic, a proliferation of researches have been done in this area \cite{lagerholm2000clustering}, \cite{sufi2011clustering}, \cite{ceylan2009novel}, \cite{bortolan1991ecg}, \cite{bortolan1993diagnostic}. However, all of these works are built on a assumption that the ECG data is small enough to fit into the memory and will not be updated, but this assumption do not hold in cardiac telehealth systems.


%The challenge derived from telehealth: Stream clustering & current works & drawbacks
As mentioned before, to provide high-quality interaction and real-time intervention for adverse cardiac events, some telehealth systems are designed to provide continuous monitoring services in a real-time manner. In these systems, the ECG data will be transmitted to remote server continuously, which makes the ECG clustering in remote server a stream clustering problem. As an important topic in data mining, plenty of literature has been proposed: BIRCH \cite{Zhang:1996:BED:235968.233324}, which uses the Cluster Feature as a compact representation of cluster, is regarded as one of the most primitive works in this area. M. Ester \textit{et al.} \cite{Ester:1998:ICM:645924.671201} address this problem by designing an incremental clustering algorithm based on DBSCAN algorithm. D. Pham \textit{et al.} \cite{pham2004incremental} combines the idea of ''cluster jumping'' with K-Means to cluster data incrementally. CluStream \cite{aggarwal2003framework} is a framework for clustering evolving data streams, which could get a clustering result during a specific time. However, none of these works have taken the speciality of the ECG data into account, thus they can not be applied in the cardiac telehealth scenario. 


%The challenge derived from telehealth: Massive data & current work
In addition, the large number of patients and the high data update rate in cardiac telehealth system converge to the fact that the data volume in the system will be extremely large, therefore a framework is required to manage such massive data. Although there are many candidates, the MapReduce framework  proposed by J. Dean \textit{et al.} \cite{dean2008mapreduce} has attracted great attention from both academia and the industry in recent years and its open-source implementation, namely Hadoop \cite{white2010hadoop}, becomes the first choice for managing big data. Although MapReduce has lots of excellent advantages, it is not easy work to build a system based on the MapReduce framework. In order to better utilize the power of MapReduce, the workflow has to be designed carefully according to the MapReduce programming model so as to distribute as much computation work as possible.


%our contribution
In this work, we designed a parallel system for clustering massive ECG stream data and implemented it on top of MapReduce. Our system performs the clustering by adopting an ECG-oriented metric and achieves the global optimum by merging and splitting dynamically. The evaluation on a Hadoop cluster with 32 computing cores shows that our system would not only provide a good clustering result but also has an excellent performance on multiple nodes.


%Arrangement of paper
The rest of this paper is organized as follows: In Section II, we present an overview of our system and indicate the challenges behind our work. In Section III, we introduce the ECG-oriented metric used throughout the whole system. In Section IV, we provide an in-depth discussion about the stream clustering algorithm. In Section V, we describe how to distribute our algorithm over Hadoop cluster. Section VI reports the evaluation result and Section VII concludes the paper.

% An example of a floating figure using the graphicx package.
% Note that \label must occur AFTER (or within) \caption.
% For figures, \caption should occur after the \includegraphics.
% Note that IEEEtran v1.7 and later has special internal code that
% is designed to preserve the operation of \label within \caption
% even when the captionsoff option is in effect. However, because
% of issues like this, it may be the safest practice to put all your
% \label just after \caption rather than within \caption{}.
%
% Reminder: the "draftcls" or "draftclsnofoot", not "draft", class
% option should be used if it is desired that the figures are to be
% displayed while in draft mode.
%
%\begin{figure}[!t]
%\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
%\caption{Simulation Results}
%\label{fig_sim}
%\end{figure}

% Note that IEEE typically puts floats only at the top, even when this
% results in a large percentage of a column being occupied by floats.


% An example of a double column floating figure using two subfigures.
% (The subfig.sty package must be loaded for this to work.)
% The subfigure \label commands are set within each subfloat command, the
% \label for the overall figure must come after \caption.
% \hfil must be used as a separator to get equal spacing.
% The subfigure.sty package works much the same way, except \subfigure is
% used instead of \subfloat.
%
%\begin{figure*}[!t]
%\centerline{\subfloat[Case I]\includegraphics[width=2.5in]{subfigcase1}%
%\label{fig_first_case}}
%\hfil
%\subfloat[Case II]{\includegraphics[width=2.5in]{subfigcase2}%
%\label{fig_second_case}}}
%\caption{Simulation results}
%\label{fig_sim}
%\end{figure*}
%
% Note that often IEEE papers with subfigures do not employ subfigure
% captions (using the optional argument to \subfloat), but instead will
% reference/describe all of them (a), (b), etc., within the main caption.


% An example of a floating table. Note that, for IEEE style tables, the 
% \caption command should come BEFORE the table. Table text will default to
% \footnotesize as IEEE normally uses this smaller font for tables.
% The \label must come after \caption as always.
%
%\begin{table}[!t]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
% if using array.sty, it might be a good idea to tweak the value of
% \extrarowheight as needed to properly center the text within the cells
%\caption{An Example of a Table}
%\label{table_example}
%\centering
%% Some packages, such as MDW tools, offer better commands for making tables
%% than the plain LaTeX2e tabular which is used here.
%\begin{tabular}{|c||c|}
%\hline
%One & Two\\
%\hline
%Three & Four\\
%\hline
%\end{tabular}
%\end{table}


% Note that IEEE does not put floats in the very first column - or typically
% anywhere on the first page for that matter. Also, in-text middle ("here")
% positioning is not used. Most IEEE journals/conferences use top floats
% exclusively. Note that, LaTeX2e, unlike IEEE journals/conferences, places
% footnotes above bottom floats. This can be corrected via the \fnbelowfloat
% command of the stfloats package.

\section{System overview and challenges}
In our system, portable sensors are deployed to collect ECG data from patients. In order to provide high-quality monitoring service and real-time intervention for adverse cardiac events, the collected ECG data will be transmitted to a remote analysis server automatically and continuously by using wireless network technologies. In the perspective of the analysis server, the ECG data will arrive as a stream, and thus as soon as the data arrives, a stream clustering is performed immediately to cluster the newly arrived data properly with respect to the existing data. The clustering result can be provided to clinicians to help in diagnosis or delivered to another system for further analysis. To achieve this, several challenges need to be conquered.


\textbf{ECG-oriented metric design:}


The primary step of cluster analysis is to define the basic processing unit. For ECG data, as it is actually a combination of electrical deflection caused by the heart beat, it can be divided into several intervals according to the heart beats, each of which is called as QRS interval in this paper and used as the basic processing unit for our cluster analysis. Figure \ref{fig_ECG_lable} presents a standard QRS interval. As showed in Figure \ref{fig_ECG_lable}, the QRS interval actually consists of P wave, QRS complex, T wave and segments between them. Each wave has some characteristic points----the beginning point, the peak and the end point. The QRS interval also has some special properties: Firstly, considering the nature of ECG, some regions of the QRS interval are more important than others. Second, since each QRS interval corresponds to a heart beat, the QRS intervals could be of various lengths. Finally, as a result of the biological diversity, the morphological characteristic of QRS intervals diversifies with different people and even with same person in different situations.
\begin{figure}[!t]
\centering
\includegraphics[height=3.0in]{./Figure/rr_interval_label.eps}
\caption{Standard ECG with labels.}
\label{fig_ECG_lable}
\end{figure}


Considering these properties, we propose an ECG-oriented metric, which defines how to measure the dissimilarity between two unequal-lengthed QRS intervals.


\textbf{Stream clustering algorithm:}


Different from the clustering of unchanged data, stream clustering poses several additional requirements: under the stream clustering scenario, data will arrive continuously, which means that the algorithm has to generate results in a very limited time. Moreover, the undergoing changes may cause existing clusters to emerge, merge and split, so the algorithm should be aware of these changes.


In our work, we introduce a new stream clustering algorithm for ECG stream data and empirically demonstrate its practical performance. The key idea of our algorithm is that as the new data arrives, it is clustered into several local clusters without initially taking the existing data into account. The connectivity and distortion of these new clusters and existing clusters will then be evaluated globally. Those clusters which have high connectivity would be merged into one, while the clusters which have high distortion would be split into smaller clusters. This approach iterates as new batch of data comes in.


\textbf{Speeding up with MapReduce:}


To conquer the challenge derived from scaling our system over massive data, we choose to implement our system on top of MapReduce. MapReduce is actually a framework for processing sophisticated parallel problems, in which the Map is used to execute independent computation on a part of the data in parallel while the Reduce is responsible for ''summing up'' the intermediate results from Map. However, due to the inherent nature of our workflow, not all the computation could be distributed, thus deciding which part to distribute becomes a vital issue for system performance.


To improve the performance as much as possible, we carefully design our workflow according to the MapReduce model in order to distribute most of the computation among the cluster. For some parts of the workflow which are dependent on the others, we break them into smaller granularity and execute the independent parts in parallel and run the dependent parts in sequence.



\section{ECG-Oriented Metric}
The primary step of clustering is to define the basic processing unit. In our system, the QRS interval is used as the basic unit. As soon as a batch of ECG data arrives, a conventional wavelet transformation based algorithm \cite{li1995detection} is applied to find the characteristic points. According to these characteristic points, the ECG data could easily be interpreted into QRS intervals. Since this algorithm has been well-discussed in \cite{li1995detection}, we will put more attention on the following steps.

The next step is to define the measurement of the dissimilarity between QRS intervals. The primary challenge deriving from the special properties of QRS interval is that they are time series data of unequal length. This makes some conventional metrics like Euclidean metric inapplicable since they can only handle data of equal length. So, to measure the dissimilarity between QRS intervals properly, we first align these QRS intervals by adopting the Dynamic Time Warping (DTW) algorithm \cite{sakoe1978dynamic}, which could make two unequal-length QRS intervals align with the same length without losing the morphological characteristic: Given two sequences of time series data $ M $ and $ N $, with length $ m $ and $ n $ respectively, the DTW algorithm would yield an optimal warping path between the two data by using the dynamic programming approach. The warping path $  WP $ is a set of tuples which is defined below:
\begin{equation}
WP = \{ (i, j) \; | \; 0 \leq i \leq m, \; 0 \leq j \leq n  \}
\end{equation}
where each tuple $ (i, j) $ in the warping path implies that the $ i^{th} $ element in the data $ M $ should be aligned with the $ j^{th} $ element in the data $ N $. So, with this warping path, these two time series data $ M $ and $ N $ could be aligned easily.


Another issue raised from the special properties of QRS interval is that considering the nature of ECG, some regions of the QRS interval are more important than others when measuring the similarity. Taking this into account, we divide the QRS interval into three segments and assign each segment with a heuristic weight, that is:
\begin{equation}\label{PT_segment}
\left\{ \begin{array}{ll}
s_1 = [P_o, Q_o], \; \textmd{weight}=w_1\\
s_2 = [Q_o, J], \;  \textmd{weight}=w_2\\
s_3 = [J, T_e], \; \textmd{weight}=w_3
\end{array} \right.
\end{equation}
where the $ P_o $ is the beginning point of P wave, $ Q_o $ is the beginning point of Q wave, $ J $ is the end point of S wave, the $ T_e $ is the end point of T wave, and the $ w_1 $, $ w_2 $ and $ w_3 $ is the heuristic weight of each segment respectively. All of these points are showed in Figure \ref{fig_ECG_lable}.


Combining these two ideas together, we get the procedure of measuring the dissimilarity between QRS intervals: Firstly, a standard QRS interval would be synthesized, and all of the QRS intervals, including the synthesized one, would be divided into three segments: $ s_1, s_2, s_3 $ according to the equation (\ref{PT_segment}). Then, the DTW algorithm is applied between each QRS interval and the standard QRS interval on $ s1 $, $ s2 $ and $ s3 $ respectively to find the warping paths. By aligning the $ s_1 $, $ s_2 $ and $ s_3 $ respectively, all the QRS intervals would be aligned with the standard QRS interval. After that, the dissimilarity between two QRS intervals can be defined as the sum of the weighted Euclidean distances of the aligned segments. 


That is, given two QRS intervals $ Q $ and $ R $, which have been aligned with the standard QRS interval, the dissimilarity $ Dis(Q, R) $ between these two QRS intervals is defined as:
\begin{equation}\label{eq_dissimilarity}
Dis(Q, R) = \sum_{i=1,2,3}{w_i * dis(s_{iq}, s_{ir} )}
\end{equation}
where $ s_{iq} $ and $ s_{ir} $ are the aligned segments $ i $ in $ Q $ and $ R $ respectively, and the $ dis(X, Y) $ is the Euclidean distance between equal-length data $ X $ and $ Y $.

\begin{figure}[!t]
\centering
\includegraphics[height=2.0in]{./Figure/algorithm_overview.eps}
\caption{Algorithm overview.}
\label{fig_algorithm_overview}
\end{figure}

\begin{figure*}[!t]
\centering
\includegraphics[width=7.1in]{./Figure/system_overview.eps}
\caption{The overview of system.}
\label{fig_sys_overview}
\end{figure*}

\section{Stream Clustering}
% the brief step of whole algorithm
% the detail of each
The design preference of providing continuous monitoring services in the new-generation cardiac telehealth system has made the data arrive as a stream. This requires the clustering algorithm not only to be able to generate results in a limited time but also be aware of the undergoing changes. Under these concerns, we present our stream clustering algorithm, which could generate results in an incremental manner by dynamically merging and splitting clusters. Figure \ref{fig_algorithm_overview} gives an overview of this algorithm. In our approach, as soon as a batch of new ECG stream data arrives, an iteration which consists of four steps would be triggered: 


Step 1 is the characteristic points extraction. In this step, a conventional wavelet transformation based algorithm \cite{li1995detection} is performed to find the characteristic points, and according to these characteristic points, the ECG data could be easily interpreted as QRS intervals, which is the basic processing unit for following steps.


The next step is the local clustering. In this step, the ECG data has already been interpreted into QRS intervals, and these QRS intervals will be divided into several groups randomly. Then, for each group, a K-Means clustering algorithm is applied to generate $ k $ clusters respectively, where the $ k $ is used to control the granularity of local clustering. After the clustering, the cluster feature of each cluster would be computed, the cluster feature $ CF $ is actually a representation of cluster, which is defined as below:
\begin{equation}
CF(C) = (c, \; e, \; r, \; lr)
\end{equation} 
where the $ c $ is the centre of cluster $ C $, $ e $ is the centre of the Euclidean space, $ r $ is the average distance between centre $ c $ and the QRS intervals in this cluster, and the $ lr $ is the maximal distance between centre $ c $ and the QRS intervals in the cluster. 

%They are defined mathematically as bellow:
%\begin{equation}\label{eq_CF_defination}
%\left\{ \begin{array}{ll}
%c = argmin \; \{ \sum\limits_{p_i \in C}{Dis(p_i, c)} \}  \\
%e = \{ (e_1, e_2, ..., e_n) | e_j = [max(p_{ij}) + min(p_{ij})] / 2  \}, \\
%r = [ \, \sum\limits_{p_i \in C}{Dis(p_i, c)} \, ] / n \\
%lr = max \; \{ Dis(p_i, c) \}
%\end{array} \right.
%\end{equation}
%where $ n $ is the number of points in cluster $ C $

In this step, some QRS intervals that should belong to the same cluster,  might be divided into different groups and thus be clustered separately, which eventually leads to their misplacement. To address this problem, the global merging and splitting is performed after the local clustering. 


Step 2 is the global merging. In this phase, we first define the connectivity $ con $ between two clusters $ Q $ and $ T $ as below:
\begin{equation}\label{eq_connectivity}
con(Q, T) = \frac{(r_q + r_t)}{Dis(c_q, c_t)} 
\end{equation}
where the $ r_q $ and $ r_t $ is the average radius $ r $ of cluster $ Q $ and cluster $ T $, the $ c_q $ and $ c_t $ is the centre of cluster $ Q $ and $ T $, and $ Dis(c_q, c_t) $ is the dissimilarity measurement between $ c_q $ and $ c_t $


All the connectivities between each pair of clusters generated from the local clustering and existing clusters generated from the previous iterations will be calculated. After that, all the clusters whose connectivities satisfy the following condition would be merged into one cluster:
\begin{equation}\label{eq_mergingCondition}
con(Q, R) \geq \lambda
\end{equation}
where $ \lambda $ is a pre-defined parameter to control the granularity of global merging.


Although the global merging would address the misplacement caused by local clustering, but it also might degrade the cluster quality by merging heterogeneous clusters together. So, after the global merging, a cluster evaluation would be performed to evaluate the quality of each cluster. The quality of cluster is represented by its distortion error, which is defined as below:
\begin{equation}\label{eq_quality}
DST(C) = \frac{w_1 * r + w_2 * Dis(c, e )}{lr}
\end{equation}
where $ DST(C) $ is the distortion error of cluster $ C $, the $ w_1 $ and $ w_2 $ are the heuristic weights, $ r $ is the average distance of cluster $ C $, $ c $ is the centre of cluster $ C $, $ e $ is the centre of Euclidean space, and $ Dis(c, e) $ is the dissimilarity measurement between $ c $ and $ e $. 


After the cluster evaluation, all the clusters which satisfy the following splitting condition would be applied with a K-Means algorithm to split into two smaller clusters:
\begin{equation}\label{eq_splittingCondition}
DST(C) \geq \tau
\end{equation} 
where $ \tau $ is a pre-defined parameter to control the granularity of global splitting. 


After the global splitting, the whole clustering iteration ends. The final clustering result will be saved in the file system for clustering next batch of data incrementally.


\section{Speedup with MapReduce}
As the data we are processing is stream data with high update rate, which implies that the data volume in our system would increase exponentially as time goes by, thus we implement our system on top of the MapReduce framework in order to handle such massive data properly. Figure \ref{fig_sys_overview} shows the workflow of our system in the perspective of MapReduce. The four steps of our stream clustering algorithm: characteristic points extraction, local clustering, global merging and global splitting, are implemented as five MapReduce jobs, and the intermediate results of each jobs are stored in HDFS as input for next job. To speed up the clustering, we designed our system carefully to distribute as much computation as possible. The following paragraphs introduce more details about our implementation.


In the workflow of our algorithm, since the workload of characteristic points extraction is relatively lightweight, it has been combined with the local clustering and implemented as a single MapReduce job. In this MapReduce job, since the characteristic points extraction and local clustering only concern about the local data stored in current machine, the computation could easily perform independently in the MAP phase, and the intermediate results are aggregated and pushed to HDFS for further use in the REDUCE phase.


However, situation becomes more complex when it comes to global merging. As shown in Figure \ref{fig_sys_overview}, the global merging is actually implemented as two MapReduce jobs: one for cluster merging and another for cluster evaluation. 


In the cluster merging, we merge clusters generated from previous local clustering step with existing clusters. Since we use the cluster feature as representation in this operation, there is not much heavy work. However, since this operation has to cross-check all the clusters to calculate the connectivity, it seems to be unable to run in parallel. After carefully investigating the workflow of cluster merging, we have found that this step could be divided into two smaller steps: scheduling and merging. The scheduling step computes the pair-wise connectivities between clusters to schedule a merging plan and the latter step conducts the actual merging according to the merging plan. Due to its serial nature, the scheduling step will only be able to run in sequence, but the merging step only involves the data containing in its assigned merging plan, so it could be performed in a single MAP phase and thus run in parallel.


After the cluster merging, the quality of each cluster has to be evaluated in order to check whether this cluster needs to be split. Although it only requires update the cluster feature for newly-generated clusters, it could not be combined with the previous MapReduce job. Recall that for a cluster feature, we have to compute the centre and the average radius of cluster. This requires to scan the whole data twice at least: the first scan finds the center, the second scan calculates the average radius with respect to this centre. However, multiple scan is forbidden in a single MapReduce job. The reason for this is that multiple scan implies the caching of whole data, but this is impossible under the scenario of massive data. As a result, we have to perform an inevitable MapReduce job for cluster evaluation after merging.


For the global splitting, like the local clustering, it only need local data stored in current machine. Therefore, this step could be easily performed in single MapReduce job. However, the quality of cluster need to be updated after splitting, thus an inevitable cluster evaluation job would be performed after the cluster splitting job.

\section{Evaluation}
To conduct the evaluation, we implemented our system on Hadoop 0.20.2 and conduct a series of experiments on a homogeneous Hadoop cluster, in which each node runs a CentOS release 5.5 and is equipped with an Intel Xeon CPU X3430 @ 2.4 GHz with 4 cores, 4 GB of RAM and 160 GB of hard disk. To get a convincing result, we cooperate with New Element Medical Ltd.,Co, Shenzhen, China \cite{szxys}, which provides us a real ECG database collected from hundreds of patients, including patients of different age, gender and heart status. In this database, there are more than 1,500 ECG data files, each of which contains about 2,800 QRS intervals. So, approximately, there are 4,356,800 QRS intervals in total. For all the experiments, we set the parameters in our system as below:
\begin{equation}
\begin{aligned}
& \{ w_1=0.15, \; w_2=0.5, \; w_3=0.35 \;  \}\\
& \{ k=4, \; \lambda=1.0, \; \tau=0.4 \;  \} 
\end{aligned}
\end{equation}
where  $ w_1 $, $ w_2 $, and $ w_3 $ are the heuristic weights used in the similarity measurement, and $ k $, $ \lambda $ and $ \tau $ are the parameter which control the granularity of local clustering, global merging and global splitting respectively.


We first conduct a baseline performance evaluation, in which we cluster an ECG stream data which consists of 1500 ECG data files and report the clustering result. Then, the scalability characteristic of our system with respect to the number of computing cores is evaluated. After that, we evaluate the performance of our system with respect to different block size of HDFS. 


\textbf{Baseline performance evaluation:}


\begin{table}[!t]\caption{Clustering Result}
\label{tb_clustering_result}
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
ID & Cluster size & Total distance & Average radius & Maximal radius\\
\hline
C1 & 2753836 & 130889825.08 & 47.53 & 186.50 \\
C2 & 1922412 & 107558951.40 & 55.95 & 202.10 \\
C3 & 2003 & 55743.49 & 27.83 & 88.04 \\
C4 & 11771 & 289684.31 & 24.61 & 110.13 \\
\hline
\end{tabular}
\end{table}

\begin{figure}[!t]
\centering
\includegraphics[height=2.7in]{./Figure/clustering_result.eps}
\caption{One of the clustering results.}
\label{fig_clustering_result}
\end{figure}

In this evaluation, we use our system to cluster an ECG stream consisted of all the ECG data files from the real ECG database. The whole data is clustered into 4 clusters and some important statistics of each cluster are listed in table \ref{tb_clustering_result}, where the cluster size is the number of QRS intervals in the cluster, the total distance is the sum of distances between the cluster centre and the QRS intervals in the cluster, the average radius is the average distance between the cluster centre and the QRS intervals in the cluster, and the maximal radius is the maximal distance between the cluster centre and the QRS intervals in this cluster. To provide an intuitive view of the clustering results, Figure \ref{fig_clustering_result} plots all the clusters in the Table \ref{tb_clustering_result} in an overlapped way. We could see that the quality of $ C1 $, $ C3 $ and $ C4 $ are good since the morphological characteristic of each QRS interval in each cluster is well-matched, even some of them have obvious drift on different segments. However, $ C2 $ has introduced some outliers. After investigating the data carefully, we find that this is caused by interference and human factor, \textit{i.e.} incorrect usage of cardiac sensors. Since we are using a real ECG database, this kind of issues are inevitable.


\textbf{Scalability with computing cores:}


To evaluate the scalability of our system, we conduct an experiments which processes 300 ECG data files in each iteration of clustering and run this experiment on a cluster scale from 8 cores to 32 cores (each node in our cluster has 4 cores, and there are 8 nodes in total). Figure \ref{fig_scalability_total_cores} shows the average running time of our system with different number of computing cores. It is obvious that the total running time is decreasing significantly as the number of computing cores grows. This shows that as the workload has been distributed and can be done in parallel, thus the performance gains a significant improvement when using more computing resource. 


Besides, the average running time of each step in our stream clustering algorithm is also evaluated. As showed in Figure \ref{fig_scalability_phase_cores}, we notice that the global merging is the most time-consuming job during the whole process. This is because the workload is relatively heavy since this step has to cross-check all the clusters in a serial manner and would causes a lot of IO operations when performing the cluster merging operation. We also find that the global merging benefits significantly when increasing the number of computing cores. This could be explained by observing the fact that the most work in the global merging is done in the reduce phase, and since the number of reducers is depend on the number of computing cores, thus if we use more computing cores, there will be more reducer running in parallel, which eventually could speed up the global merging significantly.

\begin{figure}[!t]
\centering
\includegraphics[height=1.65in]{./Figure/runningTime_cores.eps}
\caption{Scalability of PESC system.}
\label{fig_scalability_total_cores}
\end{figure}

\begin{figure}[!t]
\centering
\includegraphics[height=1.50in]{./Figure/phase_cores.eps}
\caption{Scalability of steps in stream clustering algorithm.}
\label{fig_scalability_phase_cores}
\end{figure}

\textbf{Optimization with block size: }


As mentioned at the beginning of this section, the database we are using consists of thousands of ECG data files. However, processing large volumes of small files in Hadoop could be a quite frustrating work due to its design principle \cite{white2010hadoop}. To evaluate one of the most important issue----block size of HDFS, we conduct an evaluation with different block sizes and report the result in Figure \ref{fig_block_size}. We notice that the performance gains a exponentially improvement when the block size decreases from $ 32 MB $ to $ 1 MB $, but it drops when the block size continues decreasing to $ 256 KB $. One of the most significant reasons is that for each file in the HDFS which is smaller than a block would still occupy a block. That is, as the default block size of HDFS is 64 MB, if there are 10,000 files in HDFS, each of which is smaller than 64 MB, then these files will occupy $ 10,000 * 64 MB = 625 GB $ in the HDFS, which will result in a lot of useless IO operation and networks overhead and thus eventually degrade the performance significantly. However, due to the fact that each Map task usually processes a block of input at a time, if the block size are noticeable smaller than the input files size, then there will be plenty of map tasks, each of which imposes extra bookkeeping overhead, which would also degrades the performance.
\begin{figure}[!t]
\centering
\includegraphics[height=1.65in]{./Figure/block_size.eps}
\caption{PESC performance V.S. HDFS block size.}
\label{fig_block_size}
\end{figure}



\section{Conclusion}
We have designed and implemented the PESC, a parallel system for massive ECG stream clustering. Our system would perform the clustering by adopting an ECG-oriented metric and try to achieve the global optimum by merging and splitting dynamically. The evaluation result shows that our system would not only provide a good clustering result but also produces an excellent performance on multiple computing nodes.





% conference papers do not normally have an appendix


% use section* for acknowledgement
%\section*{Acknowledgement}




% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,./ECG_Stream_Clustering_with_MapReduce}



% that's all folks
\end{document}


